\section{Appendix B: Spickzettel}

\subsection{Flussnetzwerke}

\subsubsection{Definitionen}
\begin{itemize}
	\item Ein Flussnetzwerk besteht aus einer Kapazitätsfunktion \(k\), einem gerichteten Graph \(G\), einer Quelle \(q\) und einer Senke \(s\)
	\item \textbf{Eigenschaften eines Flusses \(f:V^2 \rightarrow \mathbb{R}\)}
	\begin{enumerate}
		\item \(f \leq k\) (der Fluss kann nicht höher sein als die Kantenkapazität)
		\item \(\forall x,y \in V:~f(x,y) = -f(y,x)\) (keine negativen Flüsse)
		\item \(\forall x \in V \setminus \{ q,s\}:~0 = \sum f(x,V) = \sum_{y \in V} f(x,y)\) (Flusserhalt)
	\end{enumerate}
	\item Wert eines Flusses: \(W_f = \sum f(q,V)\)
	\item Residualgraph: Die Kanten \(f(e) < k(e)~|~e \in E\) bilden den Residualgraph \(G_f = G(V,E_f)\)
\end{itemize}

\subsubsection{Maximaler Fluss}
Der maximale Fluss ist
\begin{itemize}
	\item die Summe der von \(q\) ausgehenden Kanten,
	\item bei \(s\) ankommenden genutzten Kapazitäten oder
	\item der minimale Schnitt durch das Netzwerk.
\end{itemize}

\subsubsection{Algorithmus von Ford und Fulkerson}
\begin{itemize}
	\item Terminiert Ford-Fulkerson, so ist \(f\) maximal
	\item Aufwand: \(\mathcal{O}(|E| \cdot max\{W_f~|~f~Fluss~in~F\})\), denn ein Pfad \(q \rightarrow^{*} s\) kann in \(|E|\) Schritten gefunden werden
\end{itemize}

\subsubsection{Algorithmus von Edmonds und Karp}
Erhöhe den Fluss in \textit{FORD-FULKERSON} immer längs eines kürzesten Pfads (Breitensuche). Aufwand: \(\mathcal{O} (|E|^2 \cdot |V|)\).

\subsubsection{Präfluss-Pusch-Methode}
Jeder Knoten erhält zusätzlich eine Höhe und ein Reservoir, um vorübergehend beliebig viel Fluß speichern zu können. Aufwand: \(\mathcal{O} (|V|^2 \cdot |E|)\).

\subsubsection{Move-to-Front-Algorithmus}
Durch eine strukturierte Abarbeitung der Operationen wird \textit{Präfluss-Pusch} beschleunigt. Aufwand: \(\mathcal{O}(|V|^3)\)


\subsection{Zuordnungsprobleme}

\subsubsection{Paare in bipartiten Graphen}
TODO

\subsubsection{Paarungsvergrößerung in allgemeinen Graphen}
\begin{itemize}
	\item Suche \((x,y)\) mit ungepaartem \(x\)
	\item Setze \((x,y)\) durch Breitensuche zu einem alternierenden, maximalen Weg ungerader Länge fort
\end{itemize}

\subsubsection{Maximal gewichtete Paarungen}
Man kann eine maximal gewichtete Paarung finden, indem eine maximal gewichtete Kante iterativ durch einen vergrößernden Weg zu einer nächst größeren und maximal gewichteten Paarung vergrößert wird. Aufwand: \(\mathcal{O}(min\{|V|^3, |V| \cdot |E| \cdot log|V|\})\)

\subsection{Stochastische Algorithmen}

\subsubsection{Algorithmen}
\begin{itemize}
	\item Las Vegas Algorithmen: Falls der Algorithmus terminiert, ist das Ergbenis korrekt (nichtdeterministisch)
	\item Monte Carlo Algorithmus: Der Algorithmus terminiert immer, allerdings ist das Ergebnis mit einer bestimmten Wahrscheinlichkeit falsch
\end{itemize}

\subsubsection{Erwartungswert}
\begin{itemize}
	\item Die Zufallsvariable \(X_i\) nimmt mit einer Wahrscheinlichkeit \(p\) einen bestimmten Wert an
	\item \(\mathbb{E}\lbrack \sum X_i \rbrack = \sum \mathbb{E} \lbrack X_i \rbrack\)
\end{itemize}

\subsubsection{Bedingte Wahrscheinlichkeit}
\[Pr\lbrack E_1 | E_2 \rbrack = \frac{Pr \lbrack E_1 \cap E_2 \rbrack}{Pr \lbrack E_2 \rbrack}\]

\subsubsection{SCHNITT}
Solange die Gesamtkantenzahl \(\geq 3\) ist, wähle zufällig eine Kante \((x,y)\), entferne alle \((x,y)\) und verschmelze \(x\) mit \(y\).


\subsection{Spieltheorie}
\begin{itemize}
	\item Jedes 2-Personen-Nullsummenspiel wird durch die Spielmatrix \(M = \lbrack m_{ij} \rbrack_{i,j=1,1}^{m,n}\) beschrieben
	\item Optimale Strategie von \(A\): \(max_i~min_j~M_{ij}\) (maximiert den Mindestgewinn von \(A\))
	\item Optimale Strategie von \(B\): \(min_j~max_i~M_{ij}\) (minimiert den Maximalverlust von \(B\))
	\item Verfolgen \(A\) und \(B\) gemischte Strategien, so gilt: \(\mathbb{E} \lbrack Gewinn(A)\rbrack = p^TMq\)
\end{itemize}

\subsubsection{Minimax Theorem}
Sei $M=\begin{pmatrix}
5 & 6 \\
7 & 4 
\end{pmatrix} $ dann ist $ min_y =  \begin{pmatrix} 
x & 1-x 
\end{pmatrix}
\begin{pmatrix}
5 & 6 \\
7 & 4 
\end{pmatrix} = 0 $ Nullsetzen.\newline $max_x = 
\begin{pmatrix}
5 & 6 \\
7 & 4 
\end{pmatrix}
\begin{pmatrix}
y \\
1-y 
\end{pmatrix} = 0 $ ebenfalls Nullsetzen.
Die Nullstellen entsprechen dem Ergebnis. \newline Also ist $f(x,y)=(max_x,min_y)$ bzw. $(\begin{pmatrix}
max_x \\
1 - max_x
\end{pmatrix}
,
\begin{pmatrix}
min_y \\
1 - min_y
\end{pmatrix} )$ und der Wert ergibt sich durch Einsetzen in $ max_x \cdot min_y \cdot M=f(x,y) = 
\begin{pmatrix} 
x & 1-x 
\end{pmatrix}
\begin{pmatrix}
5 & 6 \\
7 & 4 
\end{pmatrix}
\begin{pmatrix}
y \\
1-y 
\end{pmatrix}$ und von rechts nach links ausrechnen.

\subsubsection{Von Neumanns Minimax Theorem}
\[max_p~min_q~p^TMq = min_q~max_p~p^TMq\]

\subsubsection{Yaos Technik}
Lautzeitabschätzung eines stochastischen Algorithmus \(A\) für ein Problem \(P\).
\begin{itemize}
	\item Worst-Case-Laufzeit: \(max_i~M_{ij}\)
	\item Best-Case-Laufzeit: \(min_j~M_{ij}\)
	\item Deterministische Komplexität \(K_d\) von \(P\): \(min_j~max_i~M_{ij}\)
	\item Stochastische Komplexität \(K_s\) von \(P\): \(max_i~min_j~M_{ij}\)
	\item \(K_s \leq K_d\) 
	\item Verteilungskomplexität \(K_v\): \(max_p~min_q~p^TMq\)
\end{itemize}

\subsubsection{Spielbaumauswertung}
\[Wert(x) = Wert(y) \downarrow Wert(z)\]

TODO: Algorithmus


\subsection{Algorithmen für geometrische Probleme}

\subsubsection{Binärer Raumzerlegungsbaum (RZB)}
\begin{itemize}
	\item Gerichtete Strecken \(s_1,...,s_n\) zerlegen eine Ebene \(A\)
	\item Ziel: Kleine RZBs mit wenigen Zerlegungen
	\item Aufwand: \(\mathbb{E} \lbrack~|Zerlegungen|~\rbrack < 2nlogn\)
\end{itemize}

\begin{minipage}{\textwidth}
BRZ
\begin{lstlisting}[frame=single,numbers=left,mathescape]
$k = 1$
Forall $P_i$ {
	Bestimme Schnittflaeche mit $P$
	Falls es eine Schittflaeche gibt: $k++$
}
$l = 1$
Falls es ein $j$ gibt: $Q_j$ zerlegt $P$ {
	$l = j$
}
Wurzel = $Q_l$
Falls $k \geq 3$ {
	$Q$ = Schnittmenge von $P$ und dem li. HR von $Q_l$
	linker Wurzelteilbaum = $BRZ(Q,Q_1,...,Q_{k-1})$
	$Q$ = Schnittmenge von $P$ und dem re. HR von $Q_l$
	rechter Wurzelteilbaum = $BRZ(Q,Q_1,...,Q_{k-1})$
}
Return Wurzel und Teilbaeume
\end{lstlisting}
\end{minipage}

\subsubsection{Beziehungen zwischen Knoten, Kanten und Facetten}
Für einen Polyeder mit \(v\) Knoten, \(e\) Kanten und \(f\) Seiten gilt:
\begin{itemize}
	\item \(v-e+f=2\) (Eulers Formel)
	\item \(\frac{2e}{v} \leq 6\) (mittlerer Knotengrad)
	\item \(\frac{2e}{f} \leq 6\) (mittlere Kantenzahl)
\end{itemize}

\subsubsection{Konvexe Hülle}
\textbf{Aufwand: } \(\mathcal{O}(nlogn)\)
\\\\
\begin{minipage}{\textwidth}
KONVEXE HÜLLE
\begin{lstlisting}[frame=single,numbers=left,mathescape]
# TODO anschaulich
\end{lstlisting}
\end{minipage}


\subsection{Geometrische Algorithmen}
Generell ist hierbei die Qualität der Ausgabe wichtiger als die Laufzeit. Es kann sein, dass keine oder keine eindeutige Lösung existiert.

\subsubsection{Ausweg aus einem Labyrinth}
Wir betrachten ein Labyrinth \(L\) und einen Roboter \(R\) mit Tastsensor und Drehwinkelmessgerät.
\\\\
\begin{minipage}{\textwidth}
PLEDGE-Strategie
\begin{lstlisting}[frame=single,numbers=left,mathescape]
While $R \in L$ {
	Gehe vorwaerts bis Wand kontaktiert wird
	While $R \in L$ OR Drehwinkel = $0$ {
		Gehe links der Wand
	}
}
\end{lstlisting}
\end{minipage}

\subsubsection{Zum Ziel in unbekannter Umgebung}
Gegeben sind \(P_1,...P_n\) disjunkte Polygone, ein Roboter \(r\), ein Startpunkt \(s\) und ein Ziel \(z\).
\\\\
\begin{minipage}{\textwidth}
WANZE
\begin{lstlisting}[frame=single,numbers=left,mathescape]
While $r \ne z$ {
	Laufe in Richtung $z$ bis $r == z$
			OR $r$ nicht aus einem Polygon herauskommt
	Falls $r \ne z$ {
		Suche den kuerzesten Punkt $q$ an $P_i$ zu $z$
		Laufe zu $q$
	}
}
\end{lstlisting}
\end{minipage}
Wanze terminiert.

\subsubsection{Türsuche}
\paragraph{Algorithmus} Laufe abwechselnd links und rechts einer Wand und verlängere iterativ die Laufweite.

\paragraph{Berechnung des Kompetivitätsfaktors}
Ist die Tür im ungünstigsten Fall \(d = 2^{n + \delta}\) entfernt, so gilt:
\[w \leq 2 \sum_{i=0}^{n+1} + d = 2^{n+3+\delta} + d \leq 9 \cdot 2^{n+\delta}\]

\subsubsection{Sternsuche}
\paragraph{Algorithmus} Gegeben sind \(m\) regelmäßig angeordnete Halbgeraden mit einem gemeinesamen Startpunkt \(s\), wobei sich das Ziel auf einer Halbgeraden mit unbekanntem Abstand von \(s\) befindet. Laufe reihum mit wachsender Suchtiefe, bis das \(z\) gefunden ist.

\paragraph{Berechnung des Kompetivitätsfaktors}
TODO

\subsubsection{Suche in Polygonen}
TODO?


\subsection{Lineare Programmierung}

\subsubsection{Simplex-Algorithmus}

\begin{minipage}{\textwidth}
SIMPLEX
\begin{lstlisting}[frame=single,numbers=left,mathescape]
Solange ein $c_S > 0$
	Falls alle $b_{is} \geq 0$
		Keine Loesung
	Sonst
		Bestimme $r$ so, dass $\frac{b_r}{b_{rs}} = max_{(b_{is} < 0)}~\frac{b_i}{b_{is}}$
		AUSTAUSCH
\end{lstlisting}
\end{minipage}

\begin{minipage}{\textwidth}
AUSTAUSCH(A, r, s)
\begin{lstlisting}[frame=single,numbers=left,mathescape]
Forall $i \ne r$, $j \ne s$ {
	$a_{ij}' = a_{ij} - \frac{a_{is} \cdot a_{rj}}{a_{rs}}$
}
# Pivotzeile
Forall $i == r$, $j \ne s$ {
	$a_{ij}' = -\frac{a_{ij}}{a_{rs}}$
}
# Pivotspalte
Forall $i \ne r$, $j == s$ {
	$a_{ij}' = \frac{a_{ij}}{a_{rs}}$
}
# Pivot
Forall $i == r$, $j == s$ {
	$a_{ij}' = \frac{1}{a_{rs}}$
}
\end{lstlisting}
\end{minipage}

\subsubsection{Lösen einer Spielmatrix mit Simplexalgorithmus}
\textbf{Gegeben:} \(A = \begin{pmatrix} 5 & 6 \\ 7 & 4 \end{pmatrix}\). Daraus ergibt sich das lineare Programm:
\[\begin{pmatrix} 5 & 6 & -1 & 0 \\ 7 & 4 & -1 & 0 \\ -1 & -1 & 0 & 1 \\ 0 & 0 & 1 & 5 \end{pmatrix}\]

\(\begin{pmatrix} 5 & 5 \\ 5 & 5 \end{pmatrix}\) kann zur Vereinfachung erstmal von der Ausgangsmatrix abgezogen, muss beim Ergebnis allerdings berücksichtigt werden.

\subsubsection{Ausgleichen/Minimieren mit der Maximumsnorm}
\textbf{Ziel:} Minimiere mit Hilfe der Maximusnorm \(\begin{pmatrix} y(-1) & -(-1) \\ y(0) & -1 \\ y(1) & -1 \end{pmatrix}\).
\\\\
Daraus ergibt sich das überbestimmte Gleichungssystem
\[\begin{pmatrix} 1 & -1 \\ 1 & 0 \\ 1 & 1 \end{pmatrix} \begin{pmatrix} x_1 \\ x_2 \end{pmatrix} = \begin{pmatrix} -1 \\ 1 \\ 1\end{pmatrix}.\]
\\\\
Das Gleichungssystem zeigt die invertierten und nicht-invertierten Nebenbedingungen des Linearen Programms:
\[\begin{array}{ccc|c}
	x_0 & x_1 & x_2 \\
	\hline
	-1 & -1 & -1 & 1 \\
	 1 & -1 &  0 & 1 \\
	 1 & -1 &  1 & 1 \\
	-1 &  1 & -1 & 1 \\
	-1 &  1 &  0 & 1 \\
	-1 &  1 &  1 & 1 \\
	\hline
	1 & 0 & 0 & 0 \\
\end{array}\]
Das Lineare Programm kann zeichnerisch oder rechnisch gelöst werden (siehe Klausuraufgabe 7/WS2014).

\subsubsection{Berechnung der Normalform}
Unter Verwendung der Normalform können sukszessive Zeilen in der Reihenfolge \(r=1,2,...,n\) eleminiert werden. Das der Ursprung dabei im Simplex bleibt, muss gelten:
\[a_i - \frac{a_{ir}\cdot a_r}{a_{rr}} \geq 0,~ \forall i>r\]

\paragraph{Algorithmus}
\begin{enumerate}
	\item Prüfe ob Zeile \(i\) eleminiert werden kann
	\item Wende \textit{AUSTAUSCH} an
	\item Streiche Zeile \(i\) und gehe zur nächsten Zeile
\end{enumerate}



\subsection{de Casteljau}
\[\frac{\Delta B_0}{t} \frac{\Delta B_1}{t-1} = C(\Delta B,t)\]


\subsection{Unterteilungsalgorithmen}
Das Differenzenschema existiert nur, wenn \(\alpha(z)\) den Faktor \(1+z\) besitzt oder
\[\alpha(-1) = \sum_{i \in \mathbb{Z}} \alpha_{2i} - \sum_{i \in \mathbb{Z}}\alpha_{2i + 1} = 0\]
gilt.

\subsubsection{Unterteilungsmatrizen}
Die Unterteilungsmatrizen stellen den Algorithmus dar.

\subsubsection{Symbol}
\begin{itemize}
	\item \(\alpha (z) \) ist das Symbol des Unterteilungsalgorithmus und stellt den Algorithmus dar \(\rightarrow\) kann ersetzt werden, um einen beliebigen Algorithmus zu erhalten
	\item \(c(z^2)\) ist die Ausgangsmatrix, die unterteilt wird
	\item \(b(z) = \alpha(z) \cdot c(z^2)\) ist die Unterteilungsgleichung 
\end{itemize}


\subsubsection{Differenzenschema}
\(\beta(z)\) bezeichnet das Differenzenschema zu \(\alpha\).
\[\beta(z) = \frac{\alpha(z)}{1+z}\]
\\\\
Das Differenzenschema zu \(\alpha(z)\) existiert nur, wenn \(\alpha(z)\) den Faktor \((1+z)\) hat, bzw. wenn
\[\alpha(-1) = \sum_{i \in \mathbb{Z}}\alpha_{2i} - \sum_{i \in \mathbb{Z}}\alpha_{2i+1} = 0.\]

\subsubsection{Konvergenz}
Die Funktion \(c^k(x)\) konvergiert gegen eine Funktion \(c_{\infty}(x)\) so, dass
\[sup(x \in \mathbb{R})~|~c_{\infty}(x) - c^k(x)~|~\in \mathcal{O}(4^{-k}).\]

\paragraph{Folgerung 1}
Die skalierten Differenzpolygone \(2^k d^k(x)\) (Ableitungspolygone) konvergieren gegen
\[\frac{d}{dx} c_{\infty}(x).\]

\paragraph{Folgerung 2}
Induktiv folgt aus \textit{Folgerung 1}, dass die \(n\)-te Ableitung
\[\frac{d^n}{dx^n} c_{\infty}(x)\]
der Limesfunktion stückweise konstant ist.


\subsection{Unterteilungsalgorithmen für Flächen}
Die Matrizen \(U\) und \(V\) sind stationäre Unterteilungsalgorithmen für Kurven und unterteilen das biinfinite Kontrollnetz wie folgt (Tensorproduktunterteilungsschema, Tepus):
\[B = b_{\mathbb{Z}^2} = UCV^T\]

\subsubsection{Symbole}
\begin{itemize}
	\item \(\alpha(x)\) und \(\beta(y)\) sind die Symbole von \(U\) und \(V\)
	\item \(c_{\mathbb{Z}^2}\) ist das Symbol des Kontrollnetzes
	\item \(b(x,y) = \alpha(x) \cdot c(x^2,y^2) \cdot \beta(y)\)
	\item \(\gamma(x,y) = \alpha(x) \cdot \beta(y) = \sum \gamma_{ij} \cdot x^i \cdot y^j\) ist das Symbol des neuen Unterteilungsschemas
	\item \(b(x,y) = \gamma(x,y) \cdot c(x^2,y^2)\)
\end{itemize}

\subsubsection{Masken}
Siehe Übungsaufgabe 11.1 und 11.2.


\subsection{Boyer-Moore-Algorithmus}
Siehe Algorithmen im Skript. Alphabet beachten!


\subsection{Optimierungsprobleme Lineare Programmierung}

TODO: AUSTAUSCH Algorithmus
TODO: Hässlicher Grundaufbau
TODO: SIMPLEX + Vertauschungen


\subsection{Stochastische Verfahren}
\subsubsection{Feststellung von 
Dueck,Scheuer und Wallmeister}
\begin{itemize}
	\item Schwellwert-Verfahren 
und simuliertes Tempern können aus 
kleinen, aber tiefen lokalen Minima 
entkommen, die anderen Verfahren 
nicht.
	\item Aber bei 
hoch-nicht-lineraren 
Optimierungsaufgaben mit vielen 
Parametern sind lokale Extremwerte im 
Allgemeinen weniger markant und tief.
	\item Sinflut und 
Schwellwert-Verfahren sind dem 
simulierten Tempern überlegen.
\end{itemize}


\subsection{Evolutionäre Algorithmen}
Nachkommen mutieren nach der Regel 
$q^(t+1) = q^t +d$ ,wobei d normal 
oder geometrisch verteilt mit dem 
Steuungsvektor $\sigma$ ist. Das 
Heißt 
ein Nachkomme kann um bis zu $\sigma$ 
vom Elter abweichen.

\subsubsection{ Die Plus oder ( $\mu$ 
+ 
$\lambda$ ) Evolutionsstrategie}
Zu jedem Zeitpunkt werden $\lambda$ 
Nachkommen erzeugt jedoch überleben 
nur $\mu$ Individuen der besten 
Fitness 
$c(q)$ . Eltern können überleben

\subsubsection{Komma oder 
($\mu$ , $\lambda$ 
)Evolutionsstrategie}
Gleich der Plus-Strategie, aber 
Eltern überleben nicht. Nur unter den 
Nachkommen überleben die Stärksten.

\subsection{Genetische Algorithmen}
\subsubsection{Fortpflanzung}
	Jedes Individuum q wird mit Wahrscheinlichkeit $W(q)=c(p)/ \sum_{p \in Q}c(p)$ Elter. Die $\mu$ Individuen der Population erzeugen $\mu$ Klon Nachkommen nur diese überleben.
\subsubsection{Kreuzung}
Unter den $\mu$ Nachkommen werden p\% Individuen der Kreuzung unterzogen. Zufällige Kreuzungspaare. 
$a:= (a_1, \dots , a_n)$ und $ b:=(b_1, \dots , b_n)$ ergibt $c:= (a_1 , \dots , a_j , b_{j+1} , \dots , b_n$ und $d:= ( b_1 , \dots , b_j , a_{j+1} , \dots , a_n)$
\subsubsection{Mutation}
Invertierung jedes Bits des Merkmalsvektors mit sehr niederiger Wahrscheinlichkeint $p_m$

\subsection{Vergleich Evolutionsstrategien und Genetische Algorithmen}
\subsubsection{Evolutionsstrategien}
	\begin{itemize}
		\item konvergieren im allgemeinen schneller
		\item enden öfter im lokalen Minimum
	\end{itemize}
\subsubsection{Genetische Algorithmen}
	\begin{itemize}
		\item bevorzugen Kreuzung vor Mutation
		\item spähen daher Suchraum in größeren Sprüngen aus
		\item finden öfter ein globales Optimum
		\item aber konvergieren schlechter
	\end{itemize}
	
\subsection{Partikelschwarm-Optimierung}
Ein Schwarm besteht aus Partikeln. i-ter Parikel: \begin{itemize}
	\item	Position $p_i(t)$ im $\mathbb{R}^n$
	\item	Fitness $f(p_i)$
	\item	Geschwindigkeit $v_i(t)=p_i(t)-p_i(t-1)$
\end{itemize} Dabei wird $f(x)$ optimiert.
\begin{itemize}
	\item	\textbf{Initialisiere} die $p_i(0)$ und $v_i(0)$ zufällig. Zum Zeitpunkt $t$ sei $q_i$ die bisher beste Position von $p_i$ und $q$ das bisher beste $q_i$
	\item	\textbf{Aktualisiere} damit die nächsten Positionen und Geschwindigkeiten: $v_i \leftarrow v_i + \omega + (q_i - p_i)*c_1 * r_1 + (q - p_i)*c_2 *r_2$ 
	wobei $\omega$ ,
	 $c_1$ , $c_2$
	  Lernkonstanten und $r_1$, $r_2$ Zufallszahlen in $[1,2]$
	\item	\textbf{Typische Zahlen:} 20-40 Partikel, $\omega \in [0.4,0.9]$, $c_1=c_2 \in [0,4]$
\end{itemize}

\subsection{Ameisen-System}
Seien ${1, \dots , n}^2$ die Kantenmenge eines Graphen, $d_{ij}$ Länge Kante $ij$ und $p_{ij}$ die Intensität Pheromonmarkierung.
Von $m$ Knoten aus wird je eine Ameise auf eine Rundreise geschickt danach $p_{ij}$ neu gesetzt.

$p_{ij}= \rho p_{ij} + \sum_{k}
 \Delta p_{ij}^k $
, wobei $\rho$ eine Evaporationsrate in 
$[0,1)$

$\Delta p_{ij}^k =
\left\{\begin{array}{cl} 
1/d_{ij}, & \mbox{falls Ameise k Kante ij benutzte }\\
0, & \mbox{sonst}
 \end{array}\right.$

TODO: Wenn noch Platz komplette Altklausur auf den Spickzettel packen
