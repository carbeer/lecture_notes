\chapter{Systementwurf und Implementierung}

Zusammenfassung der Vorlesung "`Systementwurf und Implementierung"' aus dem Sommersemester 2017.\footnote{\url{http://os.itec.kit.edu/deutsch/3321_3327.php}}

\section{Betriebssystemstrukturen und -schnittstellen}

\subsection{Traditionelle Betriebssystemstrukturen}
\begin{itemize}
	\item Monolithischer Kernel im privilegierten Modus
	\item (Multithreaded) Anwendungen im Benutzermodus
	\item Schnittstellen: Library API; Syscalls
\end{itemize}


\subsection{Ziele und Funktionalität}
\begin{itemize}
	\item \textbf{Aufgaben eines OS}
	\begin{itemize}
		\item Ressourcenmanagement und Accounting
		\item Hardware-Abstraktion: Erlaubt gemeinsame Datenzugriffe (Beispiel: Buffer Cache) sowie Zugriffskontrolle
		\item Fehlervermeidung
		\item Isolierung von Anwendungen inklusive Hardware-Mechanismus (Benutzermodus der CPU) zur Programmausführung (Betriebssystem muss nicht mehr jede Instruktion auf Gültigkeit überprüfen \(\rightarrow\) kein Leistungsverlust)
	\end{itemize}
	\item \textbf{Monolithische Kernel}
	\begin{itemize}
		\item Viele Abstraktionen (Prozesse, Dateien, Sockets, etc.)
		\item Schutz lediglich zwischen Prozessen und gegenüber dem Kernel (keinerlei Sicherheit innerhalb des Kernels)
		\item Alle Betriebssystemfunktionalität innerhalb des Kernels (Treiber, Netzwerstack, Dateisysteme, etc.)
		\item Historisches: Teilweise Services im Userspace (\texttt{X Server}) oder innerhalb des Kernel implementiert (\texttt{nfsd})
		\item Schnittstellen: Bibliotheksaufrufe; Systemaufrufe und (asynchrone) Signale; In-Kernel-Interfaces
	\end{itemize}
	\item \textbf{Microkernel- und Multiserver-Systeme}
	\begin{itemize}
		\item Ziel: Microkernel stellt lediglich minimale Funktionalität (Adressraum, Threading, IPC) zur Verfügung \(\rightarrow\) minimiert privilegierten Code
		\item Treiber und Services isoliert im Userspace (Bugs haben minimale Auswirkungen)
		\item Kommunikation mittels \texttt{IPC} oder Shared Memory
		\item Kernel-Schnittstellen in Multiserver-Systemen
		\begin{itemize}
			\item Kernel Subsysteme sind "`Anwendungsprogramme"'
			\item Schnittstellenentwurf bekannt von Verteilten Systemen (gleiches Szenario: Interaktion verteilter Komponenten). Probleme: Calling Conventions (beispielsweise Pointer oder Referenzen); Transparenz (lokales "`Look-and-Feel"' gewünscht; Latenz)
			\item Remote Procedure Call (RPC)
			\begin{itemize}
				\item Mittels Stubs formal definierter Funktionsaufruf. Parameter und Return-Werte müssen per IPC kopiert werden
				\item Funktionsweise
				\begin{enumerate}
					\item Client-Stub wird aufgerufen
					\item Client-Stub ordnet Parameter, baut die Nachricht und sendet diese (per Kernel-Aufruf) zum Server
					\item Server-Stub dekodiert die Nachricht und ruft die entsprechende Server-Prozedur auf
					\item Server verarbeitet die Anfrage und gibt sie an den Server-Stub zurück
				\end{enumerate}
				\item Unterschiede zu Verteilten Systemen: Kommunikation deutlich effizienter \(\rightarrow\) Stub-Code hat größen Einfluss auf Verarbeitungsgeschwindigkeit; selbe Hardware/Kernel \(\rightarrow\) einheitliche Datentypen, Endian, etc.
				\item Verwendung von \textit{Interface Definition Languages} (IDLs) zum Beschreiben und automatischen Erzeugen von Schnittstellen-Code
			\end{itemize}
		\end{itemize}
	\end{itemize}
\end{itemize}


\subsection{Structure Design Space}


\subsection{Fallstudien}

\subsubsection{Fallstudie: \texttt{L4Re RPC}}
\begin{itemize}
	\item Capability: Task-lokale Berechtigung um (Id-adressiert) auf ein Object zuzugreifen. Zugriffskontrolle implizit
	\item IPC-Gate übermittelt Aufrufe
\end{itemize}

\subsubsection{Fallstudie: Exokernel}
\begin{itemize}
	\item Motivation: Existierende Betriebssysteme zu unflexibel, umfangreich und ineffizient; Abstraktionen zu allgemein, unpassend für bestimmte Anwendungen, können nicht geändert werden
	\item Ansatz: Minimale Hardware-Abstraktion (Hardware-Ressourcen werden Anwendungen direkt zur Verfügung gestellt); konfigurierbares Ressourcen-Management (Page Table, TLB, Anwendungen können Kernelmodule laden). Betriebssystem lediglich für Schutz/Multiplexen des Ressourcenzugriffs zuständig
	\item Herausforderungen in der Forschung: Sicheres Anpassen der Betriebssystemabstraktionen durch die Anwendungen; Betriebssystemfunktionalität in Libraries (prägte den Begriff "`library OS"')
	\item \textbf{Abstraktionen}
	\begin{itemize}
		\item Allokieren/Multiplexen physischer Ressourcen wie Speicher, CPU, TLB-Einträge. Beinhaltet keine Abstraktion der Hardware
		\item Schützen der Ressourcen durch Capabilities
		\item libOS: Hardwareabstraktionen für einzelnen Konsumenten
	\end{itemize}
	\item \textbf{Schnittstellen}
	\begin{description}
		\item[libOS API:] \texttt{API} komplett anpassbar; \texttt{ABI}-Aufrufe sind \texttt{API}-Funktionsaufrufe (statt System Calls) \(\rightarrow\) weniger beteiligte Sicherheitseben pro Aufruf
		\item[Exokernel:] Schnittstelle zum Exokernel (per System Call) zum Verwalten von physischen Speicherseiten, DMA-Kanälen, IO-Geräte, TLBs, Prozessoren, Interrupt-Behandlung
	\end{description}
\end{itemize}

\subsubsection{Virtualisierung}
\begin{itemize}
	\item Idee: Simulieren virtueller Maschinen mit den selben Hardware-Schnittstellen wie physischer Maschinen
	\item \textbf{Technologie-Evolution}
	\begin{itemize}
		\item Zunächst: Trap-and-Emulate (seit den frühen 1970gern im \texttt{IBM VM/370})
		\begin{itemize}
			\item Gastbetriebssystem läuft unprivilegiert, privilegierte Instruktionen erzeugen einen \textit{Trap} und werden vom Hyperviser emuliert. Anschließend Rücksprung zur VM
			\item Formale Voraussetzung\footnote{nach Popek und Goldberg (1974)}: Alle sensitiven Instruktionen müssen privilegierte Instruktionen sein, um vom Hyperviser erkannt zu werden. Nicht erfüllt bei \texttt{x86}: Manche Instruktionen verhalten sich je nach CPU-Modus unterschiedlich (beispielsweise \texttt{pushf} und \texttt{popf})
		\end{itemize}
		\item Workarounds für \texttt{x86}
		\begin{description}
			\item[VMWare:] Ersetzen der problematischen Instruktionen durch Binary Translation zur Laufzeit
			\item[Xen:] Gastsystem wird angepasst und "`weiß"', dass es virtualisiert ausgeführt wird (Paravirtualisierung). Privilegierte Instruktionen werden durch \textit{Hypercalls} ersetzt \(\rightarrow\) Ansatz vergleichbar mit Exokernel (Multiplexen der Ressourcen, low-level Syscall-Schnittstelle)
		\end{description}
		\item Hardware-seitige Unterstützung für Virtualisierung
		\begin{itemize}
			\item Zwei CPU-Modi zum Ausführen von privilegierten Instruktionen: Physical/Supervisor/Hypervisor und Guest/Virtualized \(\rightarrow\) \textit{Guest Privileged Mode} und \textit{Host Privileged Mode}
			\item Gastbetriebssystem kann selbstständig zum \textit{Guest Privileged Mode} wechseln und beispielsweise Page-Table-Einträge modifizieren
			\item Hypervisor kontrolliert Mapping der Host-Ressourcen
		\end{itemize}
	\end{itemize}
\end{itemize}



\section{CPU und Thread Management}

\subsection{Threads und Prozesse}
\begin{itemize}
	\item Ziel: Verschiedene/unabhängige Befehlsflüsse (unterschiedliche Serveranfragen; Pausieren während auf I/O gewartet wird; QoS; etc.)
	\item \textbf{Thread- und Prozessverwaltung}
	\begin{itemize}
		\item Betriebssystemaufgaben: Verwaltung, Ausführung, Scheduling, Accounting, Ressourcen-Verwaltung
		\item Zustandshaltung pro Thread inklusive \textit{Instruction Pointer} und \textit{Stack Pointer}; Schedulung-/Accounting-Zustand; Speicherzustand; Kommunikationszustand (Wait-Queue, Send-Queue, etc.). Ressourcen (File-Pointer, etc.); Berechtigungen (\texttt{UGO})\footnote{User-Group-Owner}. Gepeichert im \textit{Thread Control Block} (TCB), dieser kann auch Pointer zum Vorgänger und zum Nachfolger speichern
		\item Basisoperationen
		\begin{description}
			\item[Create:] Erstellen (inklusive TCB) und Einfügen in \textit{Ready Queue}
			\item[Startup:] Entfernen aus \textit{Ready Queue}
			\item[Block:] Speichern des Thread-Zustands auf dem Stack; Einfügen in \textit{Wait Queue} Aktualisieren des Thread-Zustands; \texttt{resume(next\_thread)}
			\item[Signal:] Thread wird von \textit{Wait Queue} in \textit{Ready Queue} verschoben; Aktualisieren des Thread-Zustands
			\item[Resume:] Entfernen des Threads aus \textit{Ready Queue}; Laden des Registerzustands; mit Ausführung fortfahren (\textit{Instruction Pointer} zeigt auf den entsprechenden Befehl)
			\item[Finish:] Aufräumen (Stack und TCB); Finden und Weiterausführen des nächsten Thread
		\end{description}
		\item Abwägungen
		\begin{itemize}
			\item Leistungsoptimierung
			\begin{itemize}
				\item Stack wird erst beim Startup reserviert, da neue Threads eventuell nie laufen oder die für die Erstellung benötigten Ressourcen dem neuen Thread berechnet werden sollen (Accounting)
				\item Aufwandsreduzierung beim Finden von freiem Speicher durch Nutzen von \textit{Free Memory Lists} für Stack und TCB
			\end{itemize}
			\item Synchronisieren von Thread-Datenstrukturen
			\begin{itemize}
				\item Gleichzeitiger Zugriff muss serialisiert werden
				\item Leistungsmetriken: Latenz bei konfliktfreiem Zugriff; Durchsatz (Operationen pro Sekunde)
				\item Implementierungsmöglichkeiten
				\begin{description}
					\item[Single Lock:] Einzelnes Lock für gesamte Datenstruktur \(\rightarrow\) Niedrige Latenz, allerdings limitierter Durchsatz
					\item[Mehrere Locks:] Separates Lock für \textit{Ready Queue}, \textit{Wait Queue}, \textit{Free Lists}, etc. \(\rightarrow\) höhere Latenz, besseres Durchsatz
					\item[(Prozessor-)lokale Free Lists:] Exklusive Allocation Pools pro Prozessor \(\rightarrow\) verringert Zugriffskonflikte beim allokieren von TCBs oder Stacks, allerdings erhöhter Speicherverwaltungsaufwand. Pools müssen eventuell balanciert werden
					\item[Lokale Ready Queues:] Prozessor-lokale \textit{Ready Queues}. Reduziert Zugriffskonflikte; setzt ggf. Load Balancing (und Synchronisierung während Load Balancing) voraus. Heutzutage meist verwendet
				\end{description}
			\end{itemize}
		\end{itemize}
	\end{itemize}
\end{itemize}


\subsection{Thread Scheduling}
\begin{itemize}
	\item \textbf{Designziel: Trennung zwischen Policy und Mechanismus}
	\begin{description}
		\item[Mechanismus:] Zuordnung (Dispatch), Unterbrechbarkeit (Preemtion), Migration, Abrechnung (Accounting)
		\item[Policy:] Allokation, Budgetierung, Prioritäten, Dienstgüteklassen, Latenzen, etc.
	\end{description}
	\item Distributed/Hierarchical Scheduling: Scheduling umfasst ggf. mehrere Subsysteme und Schichten. Beispielsweise Scheduling von Webserver-Anfragen zu Webserver Threads sowie Scheduling von Webserver Threads zu CPUs
	\item Ggf. nehmen weitere Betriebssystembestandteile Einfluss auf Scheduling-Entscheidungen. Beispielsweise Blocking-IO, IPC, Interrupts, Exceptione, etc. (Policy und Mechanisum macnhmal schwer zu trennen)
	\item \textbf{Herausforderungen}
	\begin{description}
		\item[Komplexität:] Sehr viele unterschiedliche Umgebungen, Policies, etc.
		\item[Leistungsauswirkungen:] Scheduling innerhalb des Systems verteilt und verwoben; viele beteiligte Komponenten; Trennung von Policy und Mechanismus eventuell komplex und ineffizient
	\end{description}
	\item \textbf{Scheduling Policies}
	\begin{description}
		\item[Time-based Scheduling (prozessorbasiert):] Abhängig von Working Set, Ausführungssignatur
		\item[Energy-aware Scheduling (prozessorbasiert):] Abhängig von Prozessorcharakterisitik (Performance-Counter, Sensoren, etc.)
		\item[Affinity Scheduling (ressourcenbasiert):] Vermeidung von teuren CPU-Wechseln, die Cache-Refills verursachen
		\item[Co-/Gang Scheduling:] Kommunikationsabhängig. Beispielsweise \texttt{IPC} oder Shared Memory
		\item[Load Balancing:] Lastverteilung in Abhängigkeit von Warteschlangenlänge, Idle-Time, Kontextwechselraten
	\end{description}
	\item \textbf{Ansätze}
	\begin{itemize}
		\item Kernel-Level Scheduling (traditionell)
		\begin{itemize}
			\item Kernel verantwortlich für Management/Scheduling/Dispatching der Threads; Ausführungskontext wird vom User initiiert \(\rightarrow\) globale Sicht/Zustand
			\item Kontext mit weiteren Abstraktionen verknüpft: Schutz des Speichers; Accounting; Kommunikation; Ressourcen
			\item Analyse
			\begin{itemize}
				\item Kernel kann Scheduling-Entscheidungen in allen Anwendungen durchsetzen; wenig Overhead, da zentraler Scheduling-State; allerdings eventuell problematisch, da eine einzige Scheduling-Strategie für alle Anwendungen (QoS schwierig)
				\item Komplexe, schwergewichtige Abstraktionen für Prozesswechsel-Kontext; Speicher-Kontext; I/O-Kontext; etc.
				\item Schwierig zu erweitern, da keine Modularisierung sowie tief in den Kernel angebettet
			\end{itemize}
		\end{itemize}
		\item Application-Level Scheduling (traditionell)
		\begin{itemize}
			\item Anwendungen selbst für Thread-Scheduling verantwortlich \(\rightarrow\) müssen Management/Scheduling/Dispatcher implementieren
			\item Keine Interaktion mit anderen Subsystemen/Schichten; gut erweiterbar
			\item Analyse
			\begin{itemize}
				\item Wenig Overhead durch Abstraktionen: Ledigleich innerhalb der Anwendungen; Thread-Wechsel verursacht nur einen Wechsel des Ausführungskontext
				\item Lokale Sicht, kann Anforderungen des Betriebssystems oder andere Anwendungen nicht beachten
			\end{itemize}
		\end{itemize}
		\item Fallstudie: Scheduler Activations
		\begin{itemize}
			\item Idee: Kernel-Level Threads sind zu schwergewichtig und zu wenig erweiterbar/anpassbar; User-Level Threads zu sehr anwendungsgebunden \(\rightarrow\) Kombination beider Ansätze
			\item Meist keine Intervention des Kernel beim Thread-Scheduling nötig (reine User-Level Threads). Kernel wird erst bei blockierten Threads oder Page Faults benötigt
			\item Voraussetzung: Verteiltes Scheduling, da die der Kernel den Anwendungszustand kennen muss (wie parallel arbeitet die Anwendung?) und die Anwendungen den Scheduling-Zustand des Kernels kennen muss (wenn ein Thread blockiert)
			\item Abstraktion
			\begin{description}
				\item[Virtual Processors:] Kernel stellt Adressräumen virtuelle Multiprozessoren (VP) zur Verfügung. Adressräume können weitere VPs beim Kernel beantragen und diesen Threads zur Ausführung zuordnen
				\item[Scheduler:] Jeder Adressraum verfügt über einen dedizierten User-Level Scheduler
			\end{description}
			\item Mechanismen
			\begin{itemize}
				\item Scheduler benachrichtigt den Kernel bei Thread-Operationen, die den Prozessor betreffen
				\item Der Kernel benachricht den Scheduler bei allen Ereignissen, die den Adressspace betreffen
			\end{itemize}
			\item Zusammenfassung/Analyse
			\begin{itemize}
				\item Kombiniert Anwendungslevel- mit Kernel-Level-Scheduling
				\item Erweiterbar: Kernel lediglich für Dispatch verantwortlich, Anwender können beliebige Policies implementieren
				\item Nachteil: Bei Upcall jeweils zwei User-Kernel-Übergänge
			\end{itemize}
		\end{itemize}
		\item Fallstudie: K42 (Scheduler)
		\begin{itemize}
			\item K42: IBM Forschungsprojekt um ein allgemeines, modulares, Linux-kompatibles Betriebssystem zu entwickeln
			\item Scheduling zwischen Kernel und Anwendung ausgeteilt: Jeder Prozess besteht aus Adressraum und mindestens einem Dispatcher (vom Kernel geschedult, mehrere Dispatcher für Parallelität möglich, verschiedene Dispatcher können verschiedene Strategien implementieren)
			\item Dispatcher für Thread-Scheduling zuständig \(\rightarrow\) Anwendungen können eigene Thread-Modelle benutzen und beliebig viele Threads spawnen, ohne Kernel-Ressourcen verwenden zu müssen
			\item Resource-Domänen: Orthogonal zu Prozessen; haben eigene Ressourcen-Berechtigungen; können zum Accounting verwendet werden. Dispatcher sind jeweils an einen Prozessor gebunden und gehören zu einer \textit{Resource-Domäne} \(\rightarrow\) ermöglichen Ressourcenallokation für Mehrprozessanwendungen
		\end{itemize}
		\item Scheduling in Multi-Server Systemen
		\begin{itemize}
			\item Aufbau
			\begin{itemize}
				\item Threads repräsentieren verschiedene Entitäten (Servers, Clients, Ressourcen, etc.)
				\item Verwendung von IPC für jegliche interne Kommunikation (behandeln von Requests, Dispatching, Synchronisierung, Interrupts, etc.) \(\rightarrow\) eng mit Scheduling verknüpft
			\end{itemize}
			\item Probleme: Policy sollte von Anwendung gesteuert werden; IPC-Leistung kritisch
			\item Alternativen zu IPC: Policy-Upcalls (teuer); Kernel-Erweiterungen für Schedlung (sehr komplex, eventuell sicherheitskritisch); Heuristiken
		\end{itemize}
	\end{itemize}
\end{itemize}



\section{Speicher}

\subsection{Laden von Programmen}
\begin{itemize}
	\item \textbf{Programmausführung}
	\begin{itemize}
		\item Bestandteile eines ausführbaren Programms (Executable): Ausführungskontext (initialer Instruction Pointer, initialer Stack Pointer, Programmargumente); Speicherkontext (Code, Daten)
		\item Was wird geladen? Statischer Code/Daten (vom Linker spezifiziert und als ausführbare Date gepackt); dynamischer Code/Daten (vom Betriebssystem spezifiziert)
	\end{itemize}
	\item Executable and Linkable Format (ELF): Typischer Speicherformat. Unterteilt in ELF-Header, Program-Headers, Section-Headers, Daten
\end{itemize}


\subsection{Platzieren von Programmen}
\begin{itemize}
	\item \textbf{Direktes Laden in den physischen Speicher}
	\begin{description}
		\item[Vorteile:] Trivial; einfaches 1:1-Mapping vom virtuellen in den physischen Speicher
		\item[Nachteile:] Nur eine Instanz pro Executable gleichzeitig; muss an die richtige Adresse verlinkt werden (fehleranfällig); Fragmentierung; auf Größe des physischen Speichers begrenzt
	\end{description}
	\item \textbf{Dynamisches Verschieben}
	\begin{description}
		\item[Vorteile:] Einfaches 1:1-Mapping mit Offset vom virtuellen in den physischen Speicher \(\rightarrow\) mehrere Instanzen pro Executable möglich
		\item[Nachteile:] Performenceverlust durch positionsunabhängigen Code; Fragmentierung; auf Größe des physischen Speichers begrenzt
	\end{description}
	\item \textbf{Adressübersetzung und Segmentierung}
	\begin{description}
		\item[Vorteile:] Virtuelles Mapping des Executable in den physischen Speicher. Mehrere Instanzen pro Executable möglich; keine Relokation; kann mehr Speicher als den verfügbaren physischen Speicher verbrauchen
		\item[Nachteile:] Benötigt Übersetzungstabelle; Fragmentierung; Swapping grobkörnig
	\end{description}
	\item \textbf{Paged Virtual Memory}
	\begin{description}
		\item[Vorteile:] Page-basiertes Mapping in den physischen Speicher. Mehrere Instanzen pro Executable möglich; weder a priori Load Map noch Verschiebung noch positionsunabhängiger Code nötig; kaum externe Fragmentierung; kann mehr Speicher als den verfügbaren physischen Speicher verbrauchen
		\item[Nachteile:] Seiten-basierte Übersetzungstabelle notwendig
	\end{description}
\end{itemize}


\subsection{Fallstudien}

\subsubsection{4.3BSD on VAX-11}
\begin{itemize}
	\item Rechnerarchitektur\footnote{\url{https://de.wikipedia.org/wiki/Virtual_Address_eXtension}}, die bereits früh seitenbasierten virtuellen Speicher umgesetzt hat mit angepasstem BSD (um VMS\footnote{\url{https://de.wikipedia.org/wiki/Virtual_Memory_System}}-Performance zu erreichen)
	\item \textbf{Bestandteile}
	\begin{itemize}
		\item Core Map (globale Frame-Tabelle): Speichert jeweils Name; Liste von Pointern auf freie Elemente; Gerät+Blocknummer (bei Text-Pages); Synchronisierungs-Flags
		\item Seitenübersetzungstabelle (pro Prozess)
		\item Swap Maps (pro Prozess): Übersetzt virtuelle Seiten zu Swap-Festplattenblöcke
	\end{itemize}
	\item \textbf{Adressaufbau}
	\begin{itemize}
		\item \texttt{32 Bit} Speicheradresse, \texttt{4 GB} virtueller Speicher, \texttt{512} B Seitengröße)
		\item Segmente
		\begin{description}
			\item[P0:] Text, Data
			\item[P1:] Stack (wird "`von rechts"' aufgefüllt)
			\item[S0:] Kernelspeicher
			\item[Reserved:] Ungenutzt
		\end{description}
		\item Daten- und Stackbereich können wachsen
		\item Userspace-Seitentabellen werden im Kernel-Space gespeichert
	\end{itemize}
	\item \textbf{Erstellen neuer Prozesse}
	\begin{itemize}
		\item Erstellen der Seitentabelle
		\item Laden des Programms mittels \textit{Demand Paging Policy}\footnote{Es werden immer nur die Programmteile geladen, die gerade gebraucht werden.}
		\item Verschiedene Strategien zum Initialisieren der Seitentabelleneinträge: Fill-from-Text, Zero-Fill, Invalid, etc.
	\end{itemize}
	\item \textbf{Page-Fault-Handling bei Bounds Error}
	\begin{itemize}
		\item Normalerweise einfach zu erkennen: Zugriff auf ungültigen/nicht-existierenden Eintrag in der Seitentabelle
		\item Schwieriger bei automatisch wachsendem Stack/Heap
	\end{itemize}
	\item \textbf{Nachteile}
	\begin{itemize}
		\item Weder Shared Memory noch Memory Mapped Files noch Shared Libraries noch Copy-on-Write möglich
		\item \texttt{VAX}-spezifische, nicht-portierbare Architektur
		\item Code nicht modular
	\end{itemize}
\end{itemize}

\subsubsection{System V Release 4}
\begin{itemize}
	\item \textbf{Konzept}
	\begin{itemize}
		\item Speicherbereiche: Aufgeteilt Speicherobjekte (\textit{Segmente}), die in \textit{Regionen} abgelegt sind. Beispielsweise \texttt{text/data}, Bibliotheken, Shared Memory, Memory Mapped Files, Stacks, Kernel, etc.
		\item Jede Region ist mit einem passenden Segmenttreiber verknüpft und kann als \textit{shared} (Änderungen werden direkt geschrieben) oder \textit{private} (Änderungen erzeugen per COW eine private Kopie) markiert sein
		\item Anonymer Speicher: Privates Speicherobjekt ohne permanenten Speicher. Beispielsweise \texttt{.bss}, Stack, Heap
		\item Plattformabhängige Adressübersetzung in Hardware mit maschinen-unabhängiger Schnittstelle
		\item Prozesserstellung
		\begin{itemize}
			\item Erstellen der Segmente: Text, RO-Data (entsprechende Region des Executable); Data, Stack (anonymer Speicher)
			\item Shared Libraries werden nach Bedarf im Speicherbereich abgebildet
			\item Bei \texttt{fork()} werden lediglich die Abbildungen read-only kopiert. Änderungen dynamisch per COW
		\end{itemize}
		\item Pagefaults werden vom entsprechenden Segmenttreiber behandelt
	\end{itemize}
	\item \textbf{Analyse}
	\begin{description}
		\item[Vorteile:] Modular, portierbar; verschiedene Möglichkeiten für Shared Memory; flexibel durch Segmenttreiber
		\item[Nachteile:] Kernel benötigt durch Zustandshaltung mehr Speicher; komplex und dadurch langsam, im Allgemeinen überwiegt allerdings die neue Funktionalität gegenüber den Leistungseinbußen
	\end{description}
\end{itemize}

\subsubsection{SawMill Dataspaces (L4)}
\begin{itemize}
	\item \textbf{Konzept\footnote{\url{https://os.itec.kit.edu/downloads/publ_2001_aron-ua_sawmill-framework.pdf}}}
	\begin{itemize}
		\item Modulares Virtual-Memory-Framework für anwendungsspezifisches VMM (beispielsweise in \texttt{L4Re})
		\item Ziele: Dynamisch konfigurierbar; individuelle Policies; zur Laufzeit erweiterbar
		\item Hierarchisches VM-System: Threads können einem anderen Adressraum beliebige, eigene Seiten zur Verfügung stellen. Wahlweise per \textit{Mapping} (Seite ist in beiden Adressraumen verfügbar) oder per \textit{Granting} (Seite wird verschoben)
		\item Behandlung von Pagefaults: Ein (User-Level) \textit{Pager} pro Addressraum. Bei einem Pagefault wird ggf. direkt die entsprechende Seite zurückgegeben
	\end{itemize}
	\item \textbf{Dataspaces}
	\begin{itemize}
		\item Speicherkonzept zum Ablegen unstrukturierter Daten. Kann mit Dateien, anonymem Speicher, Frame-Buffers, etc.verknüpft sein und wird jeweils von einem \textit{Dataspace Manager} verwaltet
		\item Können (partiell) als \textit{Regions} in Adressräumen gemappt werden
	\end{itemize}
	\item \textbf{Region Mapper (RM)}
	\begin{itemize}
		\item Ein User-Level-Pager pro Task \(\rightarrow\) verwaltet die Regionen des Adressraums und behandelt Pagefaults
		\item Anpassbar, effizient
	\end{itemize}
	\item \textbf{Dataspace Operationen}
	\begin{description}
		\item[Attach/Open:] Öffnet einen Dataspace und verknüpft diesen mit einer Region
	\end{description}
	\item \textbf{Zusammenfassung}
	\begin{description}
		\item[Vorteile:] Modularisiert/flexibel/anpassbar; verteilt; einfacher User-Pager implementiertbar
		\item[Nachteile:] Overhead bei Pagefaults (mehrere Protection Domains beteiligt); großer virtueller Adressraum notwendig (kein Problem bei 64bit); komplex
	\end{description}
\end{itemize}

\subsubsection{NUMA Ausblick: Real-World physischer Speicher}
\begin{itemize}
	\item Annahme: Latenz und Durchsatz bei allen Speicher Frames gleich ("`Random Access Memory"')
	\item \textbf{Unterschiede}
	\begin{description}
		\item[Caches:] Unterschiedliche Frames werden auf unterschiedliche Cache Sets abgebildet \(\rightarrow\) Page Colors
		\item[SMP Systeme:] Cache-Kohärenzprotokolle können zu Verschmutzung der Caches führen (beispielsweise durch externe Zugriffe)
		\item[ccNUMA:]\footnote{cache-coherent Non-Uniform Memory Access} Arbeitsspeicher an unterschiedliche CPUs angebunden \(\rightarrow\) unterschiedliche Zugriffszeiten
	\end{description}
\end{itemize}



\section{I/O und Gerätetreiber}

\subsection{Überblick: I/O Subsystem}
\begin{itemize}
	\item Ziele: Anwendungen sollen für unterschiedliche Geräte nicht neu entwickelt werden müssen \(\rightarrow\) Geräteunabhängigkeit durch Standardinterface pro Geräteklasse (wird von den Gerätetreibern implementiert); Unterstützung sollen mehrere Instanzen
	\item Parallele Nutzung (Festplatte, Netzwerkgerät, etc.) vs. exklusive Nutzung (Tape, Drucker, DVD-Brenner, etc.) \(\rightarrow\) Betriebssystem muss gegenseitigen Ausschluss/Serialisierung/Verklemmungsvermeidung/etc. sicherstellen
	\item Asynchrone Operationen durch Signalisierung und \texttt{WaitForObject}
	\item Schichtenmodellbeispiel: Lesen aus Datei
	\begin{description}
		\item[User-Level I/O-Software:] Lesen aus Datei
		\item[Geräteunabhängige Betriebssystemsoftware:] Blockgeräteverwaltung
		\item[Gerätetreiber:] S-ATA-Treiber
		\item[Interrupt Service Routine:] S-ATA-Treiber ISR
		\item[Hardware] Festplatte
	\end{description}
	\item Aufgaben des Gerätetreibers: Kommunikation mit Gerät/höherer Schicht; Initialisierung des Geräts
\end{itemize}


\subsection{Hardware: Unterschiede zwischen Lehrbuch und Realität}
\begin{itemize}
	\item Vereinfachte Modelle für Leistungsbetrachtung unzureichen: "`Was passiert gerade wirklich?"'; "`Wo ist der Flaschenhals?"'
	\item \textbf{Register}
	\begin{itemize}
		\item Register des Geräte-Controllers
		\begin{description}
			\item[Schreiben:] Konfiguriert den Controller; schickt Anweisungen/Daten an den Controller
			\item[Lesen:] Läd Konfiguration/Status/Daten vom Controller
		\end{description}
		\item Registerzugang: Memory Mapped I/O (MMIO)
		\begin{itemize}
			\item Geräteregister durch Load/Store-Operationen via Speicher zugänglich \(\rightarrow\) konventionell ansteuerbar/geschützt
			\item Kompliziert durch unterschiedlich schnelle Geräte \(\rightarrow\) langsam Geräte an schnellen Bussen sollten vermieden werden
			\item Register sollten nicht gecacht werden
		\end{itemize}
		\item Registerzugang: I/O Ports
		\begin{itemize}
			\item Geräteregisterzugang mittels speziellen Instruktionen. Benötigt: Spezielle Instruktionen in Programmiersprachen sowie zusätzliche Schutzmechanismen
			\item Beispiel: \texttt{UART} bei einem Standard-PC
		\end{itemize}
	\end{itemize}
	\item \textbf{Erkennen von Geräten}
	\begin{description}
		\item[Manuell:] Geräteadressen sind bekannt und werden zur Compilezeit oder mittels Boot-Parametern gesetzt
		\item[Autoprobe:] Gerätetreiber probieren typische Adressen und validieren die Antwort
	\end{description}
\end{itemize}

\subsubsection{I/O Methoden}
\begin{itemize}
	\item \textbf{Programmiert}
	\begin{itemize}
		\item Problem: Pollen des Registers notwendig (\texttt{while (*UART\_status\_reg != ready);}) \(\rightarrow\) CPU-intensiv
		\item Memory-to-Device-Transfers: Sehr aufwendig, da CPU und Bus-intensiv, da pro Wort immer zwei Bus-Zyklen notwendig sein (für Speicherzugriff und Gerätezugriff)
	\end{itemize}
	\item \textbf{Interrupt-gesteuert}
	\begin{itemize}
		\item Signalisieren der CPU das Auftreten eines Geräteereignisses (Beispiel: Empfang eines Netzwerkpaketes)
		\item Aufwendig, da der normale Betrieb pausiert, der Interrupt behandelt und der normale Betrieb forgeführt werden muss
		\item Programmable Interrupt Controller (PIC): Multiplexen/priorisieren/puffern vieler Interrupt-Quellen in Multiprozessorsystemen. Seit \texttt{Pentium4} wird der Speicherbus mitgenutzt (viel schneller)
	\end{itemize}
	\item \textbf{DMA\footnote{Direct Memory Access}-gesteuert}
	\begin{itemize}
		\item CPU spezifiziert DMA-Transfer, DMA-Engine führt diesen ohne weitere CPU-Interaktion aus \(\rightarrow\) CPU kann während Transfer weiterarbeiten
		\item Lediglich ein einzelner Interrupt zur Benachrichtigung nötig (statt einem pro Wort)
		\item Beispiel \texttt{Intel Xeon E7-x800v2}: DMA transferiert via Last-Level-Cache \(\rightarrow\) vermeidet Cache-Write-Backs und Cache-Misses bei I/O-Puffern
		\item Beispiel \texttt{ARMv7A}: DMA ignoriert Caches; Caches ignoriert DMA-Transfers
	\end{itemize}
\end{itemize}

\subsubsection{Fallstudie: PCI}
\begin{itemize}
	\item 32bit-I/O-Bus; jedes Gerät kann als Bus-Master agieren und DMA-Transfers durchführen
	\item Nummerierung für Autokonfiguration: Erkennt Busse und Geräte
	\item Configuration Space: Definiert wichtige Geräteparameter (Gerät, Hersteller, Status, Klasse, Register, etc.)
	\item Adressbereich: Wird von der \textit{Host PCI Bridge} in den physischen Speicher gemappt (linearer Adressbereich, 32bit oder 64bit)
	\item Beispiel: Netzwerkkarte
	\begin{enumerate}
		\item Betriebssystem erkennt Gerät: \texttt{00:08.0}
		\item Betriebssystem läd die Geräte Id aus dem Configuration Space: \texttt{8086:e1000} (Intel NIC)
		\item Betriebssystem läd den passenden Gerätetreiber (\texttt{e1000})
		\item Betriebssystem erkennt die Adressbereiche (\texttt{BAR0} und \texttt{BAR1}) und allokiert die Speicher-Regionen
	\end{enumerate}
\end{itemize}

\subsubsection{Fallstudie: ARM}
\begin{itemize}
	\item I/O-Geräte werden (per manuellen Konfiguration) in den physischen Adressbereich gemappt
	\item SoC-Geräts\footnote{System-on-Chip} sind unterschiedlich: Geräte; MMIO-Adressen; physischer Adressbereich der CPU und Adressbereich des Geräts (wichtig für DMA)
	\item \textbf{Systemkonfiguration: Open Firmware Device Tree}
	\begin{itemize}
		\item Vermeidet hart-kodierte Adressen
		\item Plattformunabhängige, offene Firmware mit Forth-Interpreter
		\item Forth-Bytecode wird im PCI-Gerätespeicher abgelegt
		\item Device Tree
		\begin{itemize}
			\item Beschreibt die Hardware eines Systems; wird vom Hersteller zur Verfügung gestellt
			\item Bootloader übergibt den Device Tree ans Betriebssystem
			\item Beispiel:\\\\
\begin{minipage}{\linewidth}
\begin{lstlisting}[frame=single,numbers=left,mathescape,tabsize=4]
uart0: serial@44e0900 {
	compatible = "ti,imap3-uart";
	ti,hwmods = "uart1";
	clock-frequency = <48000000>;
	reg = <0x44e0900 0x2000>;
	interrupts = <72>;
	status = "disabled";
};
\end{lstlisting}
\end{minipage}
		\end{itemize}
	\end{itemize}
\end{itemize}


\subsection{Details}

\subsubsection{Code-Erzeugung}
\begin{itemize}
	\item Herausforderungen: Low-Level Hardware-Code ist sehr fehleranfällig und schwer zu testen; Dokumentation unpräzise/fehlerbehaftet; keine anerkannte Methodik hinsichtlich Struktur und Wiederverwendung von Code vorhanden
	\item \textbf{Device Driver IDL: Devil}\footnote{\url{https://www.usenix.org/legacy/event/osdi00/full_papers/merillon/merillon.pdf}}
	\begin{itemize}
		\item Abstrakte Beschreibung von Geräten, Ports, Registern und Gerätevariablen; Compiler generiert darauß pro Gerätevariable einen Stub
		\item Robustheit: Starke Typisierung; Sicherheitsprüfen zur Compile-Zeit und zur Laufzeit
		\item Status: In der Praxis bisher nicht verwendet; verschiedene weitere Forschungsprototypen
	\end{itemize}
\end{itemize}

\subsubsection{Aktive Gerätetreiber}
\begin{itemize}
	\item \textbf{Single-Threaded}
	\begin{itemize}
		\item Monolithisches Treibermodell mit passiven Objekten und schwer implementierbarer Kontrolllogik
		\item Aktive Gerätetreiber nutzen einen separaten Thread und explizite Kommunikation zur Verwaltung der Anfragen/Geräten/IRQs
	\end{itemize}
	\item \textbf{Multi-Threaded}
	\begin{itemize}
		\item Microkernel, beispielsweise \texttt{L4}
		\item Verwendung verschiedener Threads: Server Thread; Interrupt Service Thread, Deferred ISR Thread
	\end{itemize}
\end{itemize}

\subsubsection{Buffer Verwaltung}
\begin{itemize}
	\item \textbf{(Double) Bufferung}
	\begin{itemize}
		\item Schreibzugriff
		\begin{itemize}
			\item Die Daten werden von einem Anwendungspuffer in einen Kernel-Puffer kopiert. Letzterer hält die Daten bis der Schreibvorgang abgeschlossen ist \(\rightarrow\) Anwendung kann sofort weiterarbeiten und ihren Puffer wiederverwenden
			\item Vorteile: Schreibrate kann an Blockgröße/Geschwindigkeit des Gerätes angepasst werden; weitere Informationen können ergänzt werden (Checksumme, etc.)
		\end{itemize}
		\item Lesezugriff: Zusätzlicher Kernel-Puffer \(\rightarrow\) Daten können inklusive weitere Informationen (TCP-Port, Socket, etc.) oder als Pakete (\texttt{readline()}) übergeben werden
	\end{itemize}
	\item \textbf{Kein Puffer}
	\begin{itemize}
		\item Kopieren der Puffer (s.o.) ist ineffizient ("`unbezahlbar"' in Hochgeschwindigkeitsnetzwerken)
		\item Alternativen
		\begin{itemize}
			\item Anwendungs-Puffer wird in den Kernel gemappt und aus Anwendungsspeicher gemappt. Nachteilig, da die \texttt{API} restriktiv ist und das Betriebssystem an die verwendete Seitengröße gebunden ist
			\item Empfangspuffer wird in den Anwendungsspeicher gemappt
			\item Geteilter Empangspuffer. Nachteilig, da Anwendungen schreibend zugreifen können und keine weitere Behandlung im Kernel stattfinden kann (Checksummen, etc.)
		\end{itemize} 
	\end{itemize}
\end{itemize}

\subsubsection{Interrupt-Behandlung}
\begin{itemize}
	\item \textbf{Beispiel: Naive Implementierung eines Festplattentreibers}
	\begin{itemize}
		\item Implementierung
		\begin{enumerate}
			\item S-ATA ISR benachrichtigt Block-Layer des Betriebssystems (beispielsweise den Buffer Cache)
			\item Block-Layer benachrichtigt das Dateisystem
			\item Dateisystem benachrichtigt das virtuelle Dateisystem
			\item Virtuelles Dateisystem kopiert den Puffer in den Userspace und gibt den gerade pausierten Thread wieder frei
		\end{enumerate}
		\item Problem: Aufwendig Abarbeitung notwendig, wodurch eventuell der nächste Interrupt verpasst wird \(\rightarrow\) ISRs müssen so kurz wie möglich sein \(\rightarrow\) verzögerte Abarbeitung des Interrupts
	\end{itemize}
	\item \textbf{Lösung}
	\begin{itemize}
		\item I/O-Rate kann deutlich höher sein als Interrupts verarbeitet werden können (beispielsweise durch 10G-Ethernet) \(\rightarrow\) Interruptrate muss reduziert werden
		\item Coalescing: I/O-Geräte warten auf einen Timout oder eine bestimmte Anzahl von Ereignissen bevor sie einen Interrupt auslösen; pro Interrupt werden alle verfügbaren Ereignisse abgefragt; ISR wartet kurz ob weitere Ereignisse auftreten
	\end{itemize}
\end{itemize}

\subsubsection{I/O in Fiasco.OC}
\begin{itemize}
	\item Memory Mapped Devices: Teil des physischen Adressraums \(\rightarrow\) Zugriff per Speicherbefehlen möglich; Zugangskontrolle mittels virtuellem Speichersystem
	\item PCI-Geräte werden automatisch erkannt oder (bei ARM) per Lua-Skript konfiguriert
\end{itemize}



\section{Dateisysteme}

\subsection{Einführung}
\begin{itemize}
	\item Motivation für Dateien: Ermöglicht Speichern/konkurrierendes Zugreifen auf große Mengen persistenter Daten
	\item Zugriff entweder sequentiell (beginnend am Anfang, keine Sprünge möglich) oder wahlfrei (durch Verschieben des \textit{Seek}, wichtig für Datenbanken)
	\item Zugriff entweder durch System Calls (erzeugt eine Kopie im Speicher) oder memory mapped (direkter Zugang)
	\item \textbf{Organisation}
	\begin{itemize}
		\item Festplatte und Partitionen
		\begin{description}
			\item[Festplatte:] Besteht aus MBR\footnote{Master Boot Record}, Partitionstabelle, Partitionen
			\item[Partition:] Besteht aus Boot Block, Super Block\footnote{Teil vieler Unix-Dateisysteme, hält Verwaltungsinformationen}, Free-Space-Management, Inodes, Wurzelverzeichnis, Dateien und Verzeichnisse
		\end{description}
		\item Dateien: Allokation von Blöcken
		\begin{description}
			\item[Kontinuierlich:] Führt zu Fragmentierung
			\item[Verkettete Liste:] Pointer zur nächsten Datei werden innerhalb der Datei gespeichert. Wird beispielsweise von \texttt{FAT} verwendet
			\item[Indexierung innerhalb Inode:] Liste mit Blockadressen
		\end{description}
		\item Verzeichnisse
		\begin{itemize}
			\item Speichern unbegrenzt langer Dateinamen: In-line (direkt bei den Dateiattributen); in einem Heap (als ein gesamter Block nach allen Dateiattributen)
			\item Problem: Dynamisches Verzeichnisse mit vielen Einträgen
		\end{itemize}
	\end{itemize}
\end{itemize}


\subsection{Fallstudien}

\subsubsection{FAT}
\begin{itemize}
	\item Einfaches Dateisystem für Disketten mit weniger als \texttt{500kb} Speicherplatz aus den späten 70gern
	\item \textbf{Struktur}
	\begin{itemize}
		\item Boot Record (Superblock): Speichert Größe der Datenstrukturen
		\item Cluster\footnote{\url{https://en.wikipedia.org/wiki/Data_cluster}}: Zusammenhängende Gruppe von Sektoren auf dem Datenträger; kleinste logische Einheit, die eine Datei speichern kann (ansonsten interne Fragmentierung)
		\item File Allocation Table (FAT): Speichert den Zustand pro Cluster (frei, reserviert, verwendet, beschädigt). Bei verwendeten Einträgen werden die Daten, ein Verweis zum nächsten Cluster der Datei, und ggf. dass es sich um den letzten Cluster der Datei handelt 
		\item Clustergröße und Länge der FAT-Einträge bestimmten die maximale Dateisystemgröße
		\item Wurzelverzeichnis: Hartkodiertes, normales Verzeichnis ohne "`\texttt{..}"'
		\item Verzeichnisse
		\begin{itemize}
			\item Verzeichnisse verwalten eine Tabelle mit Dateieinträgen
			\item Struktur eines Dateieintrags: Name und Erweiterung, Attribute, Zeitpunkte (Erstellen, letzte Änderung/Zugriff), Startcluster, Dateigröße
			\item Erweiterung zur Verwendung langer Dateinamen (VFAT\footnote{Virtual File Allocation Table}): \texttt{xxxxxx\(\sim\)i.xxx}-Namen werden über mehrere Verzeichniseinträge verteilt
		\end{itemize}
		\item Öffnen von Dateien
		\begin{enumerate}
			\item Gehe zu Elternverzeichnis
			\item Erster Cluster: Suche Dateieintrag, hole Nummer des ersten Clusters, lade Cluster-Daten
			\item Weitere Cluster: Mittels FAT
		\end{enumerate}
	\end{itemize}
\end{itemize}

\subsubsection{Log-Structured File System (LFS)}
\begin{itemize}
	\item Um 1990 in Stanford entweickelt; Grundlage für viele moderne Flash-Dateisysteme (JFFS, F2FS) und COW-Dateisysteme (ZFS, Btrfs)
	\item \textbf{Motivation: Bessere Schreib-Performance}
	\begin{itemize}
		\item Verhindern mechanischer Verzögerungen durch Positionieren des RW-Kopfs und Rotation
		\item Spitzenleistung nur bei sequentiellem Zugriff erreichbar \(\rightarrow\) Spurwechselvermeidung beim Schreiben sowie Verwendung eines RAM-Caches für bei Lesezugriffen (nicht-vermeidbare Spurwechsel)
		\item Flash-Speicher: Effizienteres Überschreiben großer Blöcken
	\end{itemize}
	\item \textbf{Struktur\footnote{\url{https://en.wikipedia.org/wiki/Log-structured_file_system\#Rationale}}}
	\begin{itemize}
		\item Speichermedien werden als Ringpuffer (Circular Log) behandelt \(\rightarrow\) Schreibvorgänge finden generell am Kopf durchgeführt \(\rightarrow\) reduziert teure Spurwechsel auf ein Minimum
		\item Änderungen an Dateien erzeugen neue Dateiversionen (alte können implementierungsabhängig verfügbar gemacht werden)
		\item Unkompliziertes Wiederherstellen nach einem Crash, da ab dem letzten konsistenten Zustand wiederhergestellt werden kann
		\item Inodes werden hinter den zusammenhängenden Datenblöcken der Dateien und Verzeichnissen abgelegt \(\rightarrow\) Inodes sind über die Festplatte verteilt \(\rightarrow\) Lage der Inodes zusätzlich tabellarisch festgehalten (und im RAM gecached). Persistent ebenfalls über die Festplatte verteilt mit zusätzlicher Tabelle der Positionen der Inode-Tabellen an fester Position
		\item Dateizugriff via zweistufiger Inode-Tabelle. Beinhaltet nicht mehr Spurwechsel/Sprünge als bei traditionellen Dateisystemen
		\item Garbage-Collection
		\begin{itemize}
			\item Durch Schreiben neuer Versionen verbleiben alte ungenutzt auf der Festplatte \(\rightarrow\) Speicher muss freigegeben werden
			\item Erkennen aktueller Versionen: In Inode-Tabelle referenziert
			\item Freigeben von Speicher am Ende des Ringpuffers
			\begin{itemize}
				\item Alte Versionen werden überschrieben
				\item Aktuelle Versionen werden an den Anfang verschoben
			\end{itemize}
			\item Effiziente Aufräumstrategie notwendig
			\item Segment Summary Block: Beschreibt jeden Datenblock
		\end{itemize}
	\end{itemize}
	\item \textbf{Anwendung bei Flash-Speicher}
	\begin{itemize}
		\item Technische Voraussetzungen von Flash-Speicher: Es können nur komplette, zuvor gelöschte Blöcke geschrieben werden; Anzahl möglicher Löschvorgänge pro Block begrenzt \(\rightarrow\) LFS erfüllt diese Voraussetzungen
		\item Wahlfreier Zugriff bei Flash-Speicher möglich \(\rightarrow\) keine Notwendigkeit zur Vermeidung von Kopf-Positionierung
		\item Wear-Leveling: Verteilen der Schreibvorgänge für gleichmäßige Abnutzung
		\item Flash Translation Layer (FTL): Übersetzung logischer nach physischer Adressen
		\item JFFS2\footnote{Journaling Flash File System v2}
		\begin{itemize}
			\item Dateisysteme für Flash-Speicher \textit{ohne} FTL \(\rightarrow\) erfüllt die technischen Voraussetzungen von Flash-Speicher
			\item Imap lediglich im Speicher, das Dateisystem wird beim Einhängen vollständig gescannt (feste Position wäre entgegen \textit{Wear-Leveling})
		\end{itemize}
		\item F2FS\footnote{Flash-Friendly File System}
		\begin{itemize}
			\item Dateisystem für Flash-Speicher \textit{mit} FTL
			\item Näher an LFS: Metadaten an fester Position; zwei Checkpoint-Regionen; \textit{Node Address Table} als Imap \(\rightarrow\) überlässt dem FTL die Schreibpositionierung
		\end{itemize}
	\end{itemize}
\end{itemize}

\subsubsection{Network File System (NFS)}
\begin{itemize}
	\item Ziel: Portables, transparents Portokoll für einen einfachen Dateisystemzugang
	\item Virtual File System Layer (VFS) mit neuem Kernel-Interface; \textit{VNode} als Datenstruktur
	\item \textbf{Architektur}
	\begin{itemize}
		\item Protokoll definiert per XDR\footnote{External Data Representation (IDL)} Methoden, Argumente und Ergebnisse; verwendet synchrones RPC (Transportschicht-unabhängig)
		\item \texttt{NFSv2/v3}: Zustandlose Kommunikation \(\rightarrow\) kein Wiederherstellen nach Fehlern notwendig
		\item \textit{File Handles} als Ids
		\item Server
		\begin{itemize}
			\item Zustandslos ohne Caching \(\rightarrow\) Änderungen werden sofort geschrieben
			\item Inode Generation Number: Eindeutige Nummer, die auch bei Wiederverwendung einer Inode eindeutig ist; Bestandteil eines \textit{File Handles}
		\end{itemize}
		\item Client: Transparenter Zugang zum Server-Dateisystem mittels \texttt{<host>:<path>}-Schema mittels \textit{mount}-Protokoll
		\item NFS Dateisystem Interface
		\begin{itemize}
			\item Die meisten Dateioperationen können 1:1 auf NFS-Operationen abgebildet werden
			\item Ausnahme: Auflösen von Pfadnamen, da diese Mountpoints enthalten können \(\rightarrow\) iterative Verarbeitung, Beschleunigung durch lokalen Cache
		\end{itemize}
	\end{itemize}
	\item \textbf{Probleme}
	\begin{itemize}
		\item Benutzerdaten und Sicherheit: Durch Unix-Permissions global eindeutige und gleich UIDs und GIDs notwendig; Root-Zugang mit Root-Berechtigungen; IP-basiertes Vertrauen \(\rightarrow\) mt Kerberos in \texttt{NFSv4} gelöst
		\item Konkurrierender Zugriff: Kein Locking/Synchronisieren möglich
		\item Time-Skews, beispielsweise be \texttt{make} ohne \texttt{NTP}
	\end{itemize}
	\item \textbf{Leistungsoptimierungen}
	\begin{itemize}
		\item "`Persistenz-Adjustierung"' obwohl Caching protokollseitig verboten
		\item Asynchrone Schreibzugriffe: Lokale Client-Buffer werden erst bei \texttt{Commit} an den Server gesendet und geschrieben. Problem beispielsweise: Serverneustart zwischen asynchronem Schreiben und Committen
		\item Schnellere Folge von Lesezugriffen durch "`Weiterlesen"' oder vollständiges Laden kleiner Programme
		\item Lookup-Performance: Client-Caches, die aktualisiert werden, wenn neuerhaltene Werte nicht mehr zu gecachten Werten passen
	\end{itemize}
	\item \textbf{Änderungen bei \texttt{NFSv4}}
	\begin{itemize}
		\item Internetfähigkeit, hohe Sicherheit, cross-Platform
		\item Zustandhaftete Server für Locking, Caching und explizites Öffnen bevor Lesen/Schreiben
		\item Zuständigkeiten bei Multiserver
	\end{itemize}
\end{itemize}



\section{Modularisierung und Struktur: Benennung}

\begin{itemize}
	\item Betriebssysteme bestehen aus Komponenten (Betriebssystem, Treiber, Dateisysteme, etc.) \(\rightarrow\) diese müssen zu einem kompletten System zusammengebaut werden
	\item Ziel: Identifizierung von Komponenten (beispielsweise Libraries) und Daten (beispielsweise Objekte oder Referenzen) über Namen; Auflösung von Compiler/Linker zur Compile-Zeit (monolither Kernel) oder zur Laufzeit (Multiserver)
	\item Problem: Objekte sind zur Compile-Zeit teilweise unbekannt (Beispiel: Konkrete Id eines Threads; Speicherziel im Dateisystem). Lookup zur Laufzeit problematisch/aufwendig (Raceconditions bzw. Synchronisierung)
	\item \textbf{Definitionen}
	\begin{description}
		\item[Kontext:] Teil einer Tabelle \texttt{[Name \(\Rightarrow\) Objekt]}
		\item[Binding:] Definiert die Bedeutung eines Namens innerhalb eines Contextes. Intuitiv: Definiert, was ein Name bedeutet
		\item[Katalog:] Bindings-Tabelle zu Objekt; atomare Zugriffe
		\item[Namensauflösung:] Schlägt (Bindung-basiert) einen Namen nach. Auflösung entweder zur Compile-Zeit (einmal) oder zur Laufzeit (pro Aufruf \(\rightarrow\) Caching)
	\end{description}
	\item Namensauflösung: \textit{Resolver} übersetzt Namen eines Namespaces in einen anderen (Name im Ziel-Namespace oft low-levliger oder näher am Objekt)
	\item Closure: Verbindet Objekte, die auf andere Objekte verweisen, mittels Kontext (namensbasiert). Beispielsweise ein Compiler, der zur Compile-Zeit einen Symbolkatalog \texttt{[Name \(\Rightarrow\) Objektreferenz]} (Bindings) anlegt
	\item \textbf{Benennung als Umweg (Indirection)?}
	\begin{itemize}
		\item Identifikation von Dateien mittels Inode? Nicht eindeutig (Systemgrenzen, Hardlinks); unintuitiv für Menschen
		\item Lösung: Indirektion. Feste Namespaces werden als dynamisch Namespaces gemappt
		\item Beispiele: Shell-Variablen; Mountpoints; Geräte in \texttt{/dev}
		\item Oft Mehrstufig, beispielsweise beim Auflösen eines durch eine Variable definierten Dateipfades (Variable \(\rightarrow\) Pfad \(\rightarrow\) Dateisystem und Partition mit Mountpoint \(\rightarrow\) Partition und Inode \(\rightarrow\) Festplattenblöcke)
		\item Problem: Es kann nicht immer sichergestellt werden, dass immer das selbe Objekt referenziert wird
	\end{itemize}
	\item \textbf{Namespaces}
	\begin{itemize}
		\item Namen sind innerhalb des Namespaces eindeutig und können menschliche Bedeutung haben (Dateiname, SQL-Query, etc.)
		\item Sicherstellen der Eindeutigkeit: Zentrale Vergabe; Verteilt durch UUIDs (statistisch eindeutig); Kombinationen (bespielsweise Hostname+MAC-Adresse)
	\end{itemize}
	\item \textbf{Fallstudie: Monolithisches System oder LibOS}
	\begin{itemize}
		\item Besteht aus Source-Dateien und Header-Dateien, die zu einer Binärdatei zusammengebaut werden
		\item Typen von Namen: Variablen, Funktionsnamen, Klassennamen, Namespaces/Pakete, \texttt{typedef}, virtuelle/physische Speicheradressen, etc.
	\end{itemize}
	\item Inter-Adressbereich Namen: Namen (Pfade, Ids, Thread-Ids, Capabilities, etc.) werden von Resolvern (Compiler, IPC, Nameserver, etc. aufgelöst)
	\item \textbf{Möglichkeiten zur Namensauflösung}
	\begin{description}
		\item[Black Box:] Nicht-transparente Namensauflösung. Eventuell schwierig zu verwalten
		\item[Naming Network:] Auflösungshierarchie. Jede Komponente verweist auf die nächste Hierarchieebene (Vgl. Baum)
		\item[Linked Hierarchical Naming:] Verteilte Auflösung. Finden der Wurzel oder inter-Server-Verbindungen eventuell schwierig (Lösung: Per Hostname/ThreadId/etc. hardcodieren). In Multiserversystem kann die Wurzel nicht von Kernel vorgegeben werden (widerspricht Grundprinzip) \(\rightarrow\) wird von Parent-Task gesetzt
	\end{description}
	\item \textbf{Verteilte Namensvergabe}
	\begin{itemize}
		\item Vorgehen: Iteratives Übersetzen von Quellname über mehrere Schritte in Zielnamen. Protokoll muss ggf. mehrere Namespaces verwalten
		\item Performance-Probleme: Mehrere IPC-Anfragen notwendig \(\rightarrow\) cachen statischer Daten
		\item Konsistenz-Probleme: Gecachte Namen sind nicht mehr aktuell; Bindings existieren noch nicht oder nicht mehr. Strikte Konsisten erfordert Prüfung nach der Auflösung
	\end{itemize}
	\item \textbf{Fallstudie: L4Re-basierte Multiserversysteme}
	\begin{itemize}
		\item Zielobjekte: Sämtliche Serviceanbieter (Mikrokernel, Server-Task \(\rightarrow\) Capability zur Interaktion notwendig)
		\item Namensauflösung durch Compiler; IPC-Mechanismen; lokale Registrierung; entfernte Registrierung (Zielobjekte, Registrierungen). Pfade und vordefinierte Service Ids werden zu Capabilities aufgelöst
		\item Elternprozess definiert bei Prozesserstellung den individuellen \textit{Root Catalog} aller Capabilities (\texttt{[String \(\Rightarrow\) Capability]})
		\item Inter-Katalog-Verknüpfungen: Namensauflösung gibt Capability zu nächstem Katalog zurück (iterativ Auflösung)
	\end{itemize}
\end{itemize}



\section{Kommunikation}
\begin{itemize}
	\item Motivation: Synchronisation, Datentransfer (Shared Data oder Message-Passing), Kontrolle
\end{itemize}

\subsection{Design-Prinzipien}
\begin{itemize}
	\item \textbf{Adressierung}
	\begin{itemize}
		\item Direkt (Signals, IPC) vs. indirekt (Pipes, Mailboxes, Sockets, IPC-Gates, etc.)
		\item Unicast, Anycast, Multicast, Broadcast
		\item Modelle: Dispatcher/Worker; Mailing List; Telefon
	\end{itemize}
	\item Nachrichtenübertragung: Kopieren (benötigt mehr Ressourcen) oder per Referenz (gemeinsamer Speicher vorausgesetzt)
	\item Synchron (Telefon, RPC, etc.) vs. asynchron (komplexer; benötigt eventuell Buffering; beispielsweise E-Mail, Nachrichtenwarteschlangen, Pipes, etc.)
	\item Blockierend vs. nicht-blockierend
	\item Gepuffert vs. nicht nicht-gepuffert
\end{itemize}


\subsection{Kommunikation in monolithischen Systemen: Unix}
\begin{itemize}
	\item Zweidimensional: Horizontal (zwischen Prozessen oder Treibern) und vertikal (System Calls, Signals)
	\item \textbf{Vertikale Kommunikation}
	\begin{itemize}
		\item Auf Sicherheitsbetrachtung wird verzichtet und dem Kernel vertraut
		\item Kernel kann auf Speicher der Prozesse zugreifen, allerdings nicht umgekehrt
	\end{itemize}
	\item \textbf{Horizontale Kommunikation}
	\begin{itemize}
		\item Sicherheitsbetrachtung: Keine unbegrenzte Weitergabe von Daten und Kontrollfluss
		\item Kernel stellt Kommunikationsprimitive zur Verfügung (Geteilter Speicher, Pipes, FIFOs, IPC, Sockets, etc.)
		\item Kommunikationsprimitive in Unix
		\begin{description}
			\item[Pipe:] Unidirektional, Byte-orientiert, asynchron (Ringpuffer mit fester Größe), gepuffert, (nicht-) blockierend, Adressierung über Handle
			\item[FIFO:] Semantik wie Pipe; Adressierung über globales Handle; wird ins Dateisystem gemappt \(\rightarrow\) mit Dateisystemberechtigungen verwaltbar
			\item[IPC Semaphore:] Synchronisiert mehrere Semaphoren ("`atomatisiert"' die Standardoperationen durch atomares inkrement/dekrement)
			\item[IPC Nachrichtenwarteschlange:] asynchron; puffert Nachrichten
			\item[IPC Shared Memory:] Speicherbereiche, die zwischen Prozessen geteilt werden
		\end{description}
		\item Analyse Kommunikationsprimitive in Unix
		\begin{description}
			\item[Vorteile:] Flexibel, komfortable Schnittstelle
			\item[Nachteile:] Komplexe, schwer verständliche Semantik; Verwendung von Kernelpuffern (Größenlimitierung); Kopieraufwand
		\end{description}
	\end{itemize}
\end{itemize}


\subsection{Kommunikation in Multiserversystemen}
\begin{itemize}
	\item Kernel in viele User-Level-Subsysteme (Pager, Fileserver, Netzwerk, Treiber, etc.) mit kleinem, privilegierten Microkernel aufgeteilt
	\item \textbf{Vertikale Kommunikation}
	\begin{itemize}
		\item User/Server \(\leftrightarrow\) Microkernel
		\item Server \(\leftrightarrow\) Microkernel
	\end{itemize}
	\item \textbf{Horizontale Kommunikation}
	\begin{itemize}
		\item Benutzer \(\leftrightarrow\) Benutzer
		\item Server \(\leftrightarrow\) Benutzer
		\item Server \(\leftrightarrow\) Server
	\end{itemize}
	\item \textbf{Herausforderungen}
	\begin{itemize}
		\item Komplexe Sicherheits/Isolierungs-Anforderungen (Lösung: Kernel stellt IPC zur Verfügung)
		\item Modularisierung führt zu erhöhtem Kommunikationsaufwand \(\rightarrow\) Kommunikation muss so wenig Overhead erzeugen wie möglich
		\item Sichere Abstraktion für Kommunikation mit Geräten, da Treiber nicht explizit vertrauenswürdig sind \(\rightarrow\) Zugriffskontrolle per DMA und IOMMUs
	\end{itemize}
	\item \textbf{Fallstudien}
	\begin{itemize}
		\item Mach
		\begin{itemize}
			\item Nachrichten werden im Kernel gepuffert. Ermöglicht asynchrones, nicht-blockierendes Senden (bis der Puffer voll ist)
			\item In-Line-Data: Wird indirekt zwischen Sender und Empfänger kopiert; richtige Reihenfolge garantiert
			\item Out-of-Line-Data: COW-Mechanismus \(\rightarrow\) vermeidet Kopieren großer Puffer
		\end{itemize}
		\item L4
		\begin{itemize}
			\item Synchrones, ungepuffertes, rendezvoud-basiertes IPC; blockierend oder nicht-blockierend
			\item IPC-Typen: Register-IPC (Synchronisierung); String-IPC (kopiert Speicher); Map IPC (schickt Speicherseiten aus dem eigenen Speicherbereich; potentielle Verletzung von Zugriffsrechten; Kernel aktualisiert MMU während IPC)
			\item Asychrones (gepuffertes) IPC muss mittels Threads auf User-Level implementiert werden
			\item Hardware-Abstraktion durch IPC (Paging, Interrupts, Exceptions)
		\end{itemize}
	\end{itemize}
\end{itemize}


\subsection{Kommunikation in Multiprozessorsystemen}
\begin{itemize}
	\item Probleme bei Kernel-IPC: Architekturlimitierung durch Speicherbereichwechsel; Störung der User-Level Thread-Verwaltung
	\item Mögliche Lösungen: Empfänger auf anderer CPU; Shared Memory für Nachrichtenaustausch
	\item \textbf{URPC}
	\begin{itemize}
		\item Weder Kernel-Aufruf noch Kontextwechsel
		\item Bidirektionale (Kopier-)Warteschlange via Shared Memory (asynchron, nicht-blockierend, gepuffert)
	\end{itemize}
	\item \textbf{Fallstudie: Barrelfish}
	\begin{itemize}
		\item Tread-System: Scheduler Activations; K42-artigen Dispatcher
		\item Kommunikation zwischen Dispatchern via Betriebssystem
		\item Unified-RPC-Schnittstelle mit generierten RPC-Stubs und Verbindungstreibern mit speziellen Schnittstellen
		\item Aufbau eines Kommunikationskanals
		\begin{enumerate}
			\item Nachschlagen im \textit{Lookup Server}
			\item Aufhandeln des Verbindungstreibers
			\item Weiter (spezifische) Initialisierung
		\end{enumerate}
	\end{itemize}
\end{itemize}



\section{Koordination}
\begin{itemize}
	\item Motivation: Verhindern von Raceconditions, da Operationen auf Variablen/Datenstrukturen keine atomaren Speicheroperationen sind
	\item Lösungsansatz 0: Einprozessorsystem ohne Preemtion (beispielsweise durch Interrupts)
	\item Lösungsansatz 1: Zentraler Koordinator zur Ressourcenverwaltung
	\item \textbf{Lösungsansatz 2: Locking}
	\begin{itemize}
		\item Explizites Reservieren/Freigeben von Ressourcen/kritischen Sektionen
		\item Granularität: Grobgranularere Locks sind einfacher implementiert und führen zu weniger Verwaltungsaufwand, allerdings auch zu potentiell mehr wartenden Threads
		\item Reader-Writer-Locks
		\begin{itemize}
			\item Erlaubt mehrere parallele Lesezugriffe
			\item Implementierung meist mittels Semaphoren
			\item Lesepräferenz (erlaube weitere Lesezugriffe solange mindestens ein Lesezugriff stattfindet); Schreibpräferenz (blockiere Lesezugriffe während ein Schreibzugriff wartet); erlaube alle Lesezugriffe gemeinsam während zwei Schreibzugriffen
			\item Generell gilt: Weniger komplexe Lockstrukturen sind in der Praxis oft am schnellsten \(\rightarrow\) zunächst einfache Spinlocks probieren\footnote{Hill's Law}
		\end{itemize}
	\end{itemize}
\end{itemize}


\subsection{Implementierungen}
\begin{itemize}
	\item \textbf{Zentraler Server}
	\begin{itemize}
		\item Anfordern von Locks via Nachrichten. Im einfachsten Fall (single-threaded Server) erfolgt die Synchronisation durch die Serialisierung der Nachrichten
		\item Beispiel L4: Jegliche Synchronisierung oberhalb des synchronen IPC
	\end{itemize}
	\item \textbf{Blocking (Mutex)}
	\begin{itemize}
		\item Atomare Operationen für \texttt{aquire()} und \texttt{release()}; blockierte Threads werden in eine Warteschlange verschoben; beim Verlassen des kritischen Bereichs wird der nächste Thread aufgeweckt
		\item \texttt{aquire()} und \texttt{release()} sind selbst kritische Bereiche
	\end{itemize}
	\item \textbf{Busy Waiting (Spinlock)}
	\begin{itemize}
		\item Thread pollt das Lock bis es frei ist \(\rightarrow\) atomares Locken (in Hardware) vorausgesetzt
		\item Verschwendung von CPU-Zyklen bis das Lock frei ist
	\end{itemize}
	\item \textbf{Blocking vs. Spinning}
	\begin{itemize}
		\item Deutlich höherer Verwaltungaufwand bei Blocking (blockieren, in Warteschlange einfügen, Kontextwechsel zum anderen Thread, aus der Warteschlange nehmen und aufwecken, Kontextwechsel zurück) \(\rightarrow\) typische Wartezeit entscheident. Spinlocks bei kurzen Wartezeiten, Mutex bei langen
		\item Triviale Fälle
		\begin{description}
			\item[Einprozessorsystem:] Immer blockieren. Wartender Thread wird erst wieder gestartet, wenn wieder freigegeben
			\item[Multiprozessor, kein anderer Thread wartet auf den Prozessor:] Immer spinnen, da ohnehin nichts anderes ausgeführt werden kann
			\item[Lock-Halter läuft gerade nicht:] Immer blockieren und möglichst schnell CPUs "`leeren"' um Lock-Halter weiterlaufen zu lassen (Lock-Holder-Preemtion beachten)
		\end{description}
		\item Spannende Fälle
		\begin{itemize}
			\item Lock-Halter läuft gerade auf einer anderen CPU; andere Threads warten in der CPU-lokalen Warteschlange
			\item Algorithmen
			\begin{itemize}
				\item Optimaler Offline-Algorithmus: Kennt die Wartezeiten im Voraus \(\rightarrow\) blockieren, falls die Wartezeit größer als die Blockierungskosten (zwei Kontextwechsel, Warteschlangenverwaltung, Aufwecken) sind, sonst spinnen
				\item Online-Algorithmen
				\begin{itemize}
					\item Muss Entscheidung ohne vollständige Informationen fällen
					\item Ansatz: Spinning bis eine bestimmte Wartedauer überschritten wurde, dann blockieren
					\item Bestimmen der Wartezeit
					\begin{description}
						\item[Fest:] Auf Basis der (halben) Dauer für den Blockierungsaufwand
						\item[Adaptiv:] Schätzen der durchschnittlichen Wartezeit (Radom-Walk, Sliding-Window, etc.)
					\end{description}
				\end{itemize}
			\end{itemize}
		\end{itemize}
	\end{itemize}
\end{itemize}

\subsubsection{Implementierung von Spinlocks: Atomare Instruktionen}
\begin{itemize}
	\item Anforderung: CPU garantiert atomare lese-ändere-schreibe-Sequenz (Bus-Locking; lokale Cache-Zeile, die erst nach vollständige Ausführung zurückgeschrieben wird)
	\item Typische atomare Instruktionen: compare-and-swap, swap, test-and-set, fertch-and-add
	\item \textbf{Fallstudie: x86-Atomics}
	\begin{itemize}
		\item Lese-ändere-schreibe-Sequenz mittels exklusiv gelockter L1-Cache-Zeile
		\item Triviale Nutzung
		\item \texttt{atomic\_swap()}, \texttt{atomic\_add()}, etc. können mittels Inline-Assembler definiert werden
	\end{itemize}
	\item \textbf{Fallstudie: RISC-Atomics}
	\begin{itemize}
		\item Trennung von \textit{Load} und \textit{Store} (Load Linked / Store Conditional \(\rightarrow\) LL/SC)
		\item \textit{Load} markiert Speicherregion und Cache-Zeile; \textit{Store} überprüft Markierung und gibt zurück, ob der Vorgang erfolgreich war
		\item Nachteil: Ggf. mehrere Versuche notwendig \(\rightarrow\) schwieriger zu verwenden
		\item Unter verschiedenen Namen u.a. in Alpha, ARM, MIPS und PowerPC implementiert
	\end{itemize}
\end{itemize}

\subsubsection{Implementieren von Spinlocks: Strategien}
\begin{itemize}
	\item \textbf{Wiederholtes Test-and-Set}
	\begin{itemize}
		\item \texttt{while(\_\_sync\_lock\_test\_and\_set(lock, 1));}
		\item ccNUMA: Cache-Zeile wird zwischen CPU-Kernen hin und her geschoben
		\item Performance sinkt mit wachsender Anzahl wartender Threads 
	\end{itemize}
	\item \textbf{Wiederholtes Lesen (Test-and-Test-and-Set)}
	\begin{itemize}
		\item Wartende Threads lesen nur \texttt{lock}: \texttt{while(lock || test\_and\_set(lock, 1));}
		\item ccNUMA: Cache-Zeile von den wartenden CPU-Kernen geteilt und bei \texttt{release()} invalidiert \(\rightarrow\) erzeugt einen Cache-Miss und gibt ein neues Lock zurück
		\item Problem: Wettbewerb zwischen Erkennen des freien Locks und \texttt{aquire()} (mehrere CPU-Kerne probieren test-and-set) \(\rightarrow\) nicht-erfolgreiches Test-and-Set invalidiert Lesekopie im Cache \(\rightarrow\) unnötiger Cache-Kohärenzaufwand
		\item Lösung: Benachrichtige nur einen CPU-Kern; invalidiere Cache lediglich bei Zustandswechsel des Locks
	\end{itemize}
	\item \textbf{Queue Locks}
	\begin{itemize}
		\item Idee: Wartende CPUs kommen in eine Warteschlange; es wird immer nur die erste benachrichtigt
		\item Implementierungsalternativen
		\begin{itemize}
			\item Separates Flag pro wartender CPU: Typischerweise bei ccNUMA
			\item Verteilte Schreibkohärenz mit Ticket-Weitergabe in einer einzigen Bus-Transaktion
		\end{itemize}
		\item Vergleich mit einfachem Spinlock: Höhere Latenz ohne Stau, niedrigere bei Stau
	\end{itemize}
\end{itemize}

\subsubsection{Spinlock-Analyse}
\begin{itemize}
	\item Verwendung von Spinlocks in Anwendungen mit wenig Konkurrenz, ansonsten Queue-Locks \(\rightarrow\) Benchmarks!
	\item Generell: Keine Fairness bei Spinlocks
\end{itemize}
